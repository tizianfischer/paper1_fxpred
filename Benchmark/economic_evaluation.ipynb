{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affected-orleans",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# %% Loading Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_erIPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')ror\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.activations import tanh\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import MSE, MAE, MAPE\n",
    "from tensorflow.keras.metrics import Accuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.losses import kullback_leibler_divergence, SparseCategoricalCrossentropy, KLDivergence\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import sys\n",
    "\n",
    "from tensorflow.python.ops.gen_batch_ops import batch\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "from tensorflow.python.ops.gen_math_ops import Tanh\n",
    "if platform.node() in ['msbq', 'msdai']:\n",
    "    os.chdir('/home/ms/github/fxpred')\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'Transformer'))\n",
    "from utils import get_fx_and_metric_data_wo_weekend, mde\n",
    "from utils_NN_opt_learning_rate import opt_learn_rate_plot\n",
    "from benchmark_utils import actual_pred_plot, ts_train_test_normalize\n",
    "from transformer_architecture_elements import Time2Vector, SingleAttention, MultiAttention, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'Benchmark/trained_models/LSTM_univariate2_classification____0100.hdf5'  # Classification model\n",
    "# filename = 'Benchmark/trained_models/LSTM_univariate2_with_close_price____0150.hdf5'  # Regression model\n",
    "# filename = 'content/Transformer+TimeEmbedding2_minmax____0300.hdf5'\n",
    "filename = 'content/FXTransformerMulti0001.hdf5'\n",
    "# filename = 'content/Transformer+TimeEmbeddingClassification0010.hdf5'\n",
    "\n",
    "my_LSTM_model = tf.keras.models.load_model(\n",
    "    filename,\n",
    "    custom_objects={\n",
    "        'kl_divergence': kullback_leibler_divergence,\n",
    "        'Time2Vector': Time2Vector,\n",
    "        'SingleAttention': SingleAttention,\n",
    "        'MultiAttention': MultiAttention,\n",
    "        'TransformerEncoder': TransformerEncoder\n",
    "    }\n",
    ")\n",
    "lag = my_LSTM_model.input_shape[1]\n",
    "print(f'lag: {lag}')\n",
    "if my_LSTM_model.layers[-1].activation == tf.keras.activations.softmax:\n",
    "    nn_type_classification = True\n",
    "    h = 1\n",
    "else:\n",
    "    nn_type_classification = False\n",
    "    h = my_LSTM_model.output_shape[1]\n",
    "print(f'h: {h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(my_model, X_test, sc=None):\n",
    "    LSTM_prediction = my_model.predict(X_test)\n",
    "    if sc is not None:\n",
    "        LSTM_prediction = sc.inverse_transform(LSTM_prediction)\n",
    "    return LSTM_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = np.float32  # np.float64\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "df = get_fx_and_metric_data_wo_weekend(dtype=dtype)\n",
    "target_price = target = 'EURUSD BGNE Curncy Bid Close'\n",
    "df_close = get_fx_and_metric_data_wo_weekend(dtype=dtype, pct_change=False)\n",
    "target_column = list(df.columns).index(target)\n",
    "if my_LSTM_model.input_shape[-1] <= 2:\n",
    "    df = df.iloc[:, target_column : target_column + 1]\n",
    "target_column = list(df.columns).index(target)\n",
    "target_price_column = target_column\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy(x):\n",
    "    conditions = [\n",
    "        (x <= -0.00000001),\n",
    "        (x > -0.00000001) & (x <= 0.00000001),\n",
    "        (x > 0.00000001)\n",
    "    ]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition #!!! \n",
    "    values = ['sell', 'hold', 'buy']\n",
    "    values = [0, 1, 2]\n",
    "\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments #!!! \n",
    "    return np.select(conditions, values).astype(np.int32)\n",
    "df['strategy'] = strategy(df[target_price])\n",
    "\n",
    "if nn_type_classification:    \n",
    "    target = 'strategy'\n",
    "target_column = df.columns.to_list().index(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target_price].plot()\n",
    "plt.title('Return of closing price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val , X_test, y_test, sc, sc_target, index_train, index_val, index_test, X_train_index, y_train_index = \\\n",
    "    ts_train_test_normalize(df, lag, h, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nn_type_classification:\n",
    "    y_train = sc_target.inverse_transform(y_train)\n",
    "    y_val = sc_target.inverse_transform(y_val)\n",
    "    y_test = sc_target.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-saskatchewan",
   "metadata": {},
   "source": [
    "## Econmetric measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-withdrawal",
   "metadata": {},
   "source": [
    "Definition of cumulated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulated_return(y_pred, ret, start_capital=1000, dtype=None):\n",
    "    assert y_pred.flatten().shape == ret.flatten().shape\n",
    "    balance = [start_capital]\n",
    "    signal = 0\n",
    "    for i, d in enumerate(y_pred[:-1]):\n",
    "        signal = signal if d == 1 else d\n",
    "        if signal == 0:\n",
    "            balance.append(balance[-1])\n",
    "        elif signal == 2:\n",
    "            balance.append(balance[-1] * ret[i])\n",
    "        else:\n",
    "            raise ValueError()\n",
    "    balance = np.array(balance, dtype=dtype)\n",
    "    return balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! for testing purposes\n",
    "tmp_type = 'train'\n",
    "\n",
    "X = np.concatenate([sc.inverse_transform(i)[np.newaxis] for i in eval(f\"X_{tmp_type}\")], axis=0)\n",
    "for i in range(lag):\n",
    "    diff = np.abs(df.loc[eval(f\"index_{tmp_type}\")].iloc[i:-(lag - i + 1)].values - X[:,-(lag - i)]).sum(axis=0)\n",
    "    assert diff[0] < 1e-6 and diff[1] < 0.5, 'df and X inversed are not the same'\n",
    "\n",
    "X_ret_testing = np.concatenate([sc.inverse_transform(i)[np.newaxis] for i in eval(f\"X_{tmp_type}\")], axis=0)[:, -1, 0]\n",
    "y_testing = np.concatenate([sc.inverse_transform(i)[np.newaxis] for i in eval(f\"X_{tmp_type}\")], axis=0)[:, -1, 1]\n",
    "\n",
    "a1 = sc_target.inverse_transform(eval(f\"y_{tmp_type}\")).flatten()\n",
    "a2 = eval(f\"y_{tmp_type}\").flatten()\n",
    "b = np.concatenate([sc.inverse_transform(i)[np.newaxis] for i in eval(f\"X_{tmp_type}\")], axis=0)[:, -1, 1]\n",
    "cc = df.loc[eval(f\"index_{tmp_type}\")].iloc[(lag - 1):-(h+1), 1].values\n",
    "print(np.abs(cc[1:] - a1[:-1]).sum(axis=0))\n",
    "print(np.abs(cc[1:] - a2[:-1]).sum(axis=0))\n",
    "print(np.abs(cc - b).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((index_train[0:-(lag+1)] == X_train_index[:, 0]).all())\n",
    "print((index_train[(lag-1):-(h+1)] == X_train_index[:, -1]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-scott",
   "metadata": {},
   "source": [
    "## read in data for econmic metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def econ_metrics(y_pred, X_ret, risk_free=0.0031):  # Risk free rate must be given\n",
    "    x = cumulated_return(y_pred[:-1], X_ret[1:], dtype=dtype)\n",
    "    print(f'Portfolio return: {x[-1] / x[0] - 1}')\n",
    "    print(f'Std of returns: {np.std(x[1:] / x[:-1])}')\n",
    "    print(f'Sharpe ratio: {((x[-1] / x[0]) - 1 - risk_free) / np.std(x[1:] / x[:-1])}')\n",
    "    print()\n",
    "    \n",
    "    n = 6 * 24 * 252  # banking days per year\n",
    "    mu_annum = ((x[-1]/x[0]) ** (n / x.shape[0])) - 1\n",
    "    print(f'Return per annum: {mu_annum}')\n",
    "    std_annum = np.std(x[1:] / x[:-1]) * (n)**0.5\n",
    "    print(f'Std of returns per annum: {std_annum}')\n",
    "    print(f'Sharpe ratio per annum: {(mu_annum - risk_free) / std_annum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[-1] == my_LSTM_model.input_shape[-1]\n",
    "print(X_train.shape[-1])\n",
    "print(my_LSTM_model.input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_output2(tmp_type):\n",
    "    print(tmp_type)\n",
    "\n",
    "    y = eval(f\"y_{tmp_type}\")\n",
    "    if nn_type_classification:\n",
    "        y_pred = np.argmax(predictions(\n",
    "            my_LSTM_model,\n",
    "            eval(f\"X_{tmp_type}\")\n",
    "        ), axis=1)\n",
    "    else:\n",
    "        y = strategy(y)\n",
    "        if X_train.shape[-1] - 1 == my_LSTM_model.input_shape[-1]:\n",
    "            i = df.columns.to_list().index('strategy')\n",
    "            y_pred = strategy(predictions(\n",
    "                my_LSTM_model,\n",
    "                np.delete(eval(f\"X_{tmp_type}\"), i, -1)\n",
    "            ))\n",
    "        else:\n",
    "            y_pred = strategy(predictions(\n",
    "                my_LSTM_model,\n",
    "                eval(f\"X_{tmp_type}\")\n",
    "            ))\n",
    "    print(f'Accuracy: {Accuracy()(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "    print(f'Accuracy: {SparseCategoricalAccuracy()(y.astype(dtype), y_pred.reshape(y.shape).astype(dtype)).numpy()}')\n",
    "    print(f'KL_Divergence: {kullback_leibler_divergence(y.flatten().astype(dtype), y_pred.flatten().astype(dtype)).numpy()}')\n",
    "    print(classification_report(y, y_pred))\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    plt.show()\n",
    "\n",
    "    X_ret = sc.inverse_transform(eval(f\"X_{tmp_type}\"))[:, -1, target_price_column] + 1\n",
    "    plt.plot(eval(f\"index_{tmp_type}\")[lag-1:-(h+1)][1:], cumulated_return(y_pred[:-1], X_ret[1:], dtype=dtype), label='predicted')\n",
    "\n",
    "    c = Counter(y.flatten())\n",
    "    for i in range(3):\n",
    "        c.update({i: 0})\n",
    "    p = [i[1]/y.shape[0] for i in sorted(c.items(), key=lambda x: x[0])]\n",
    "    x = np.concatenate(\n",
    "        [\n",
    "            cumulated_return(\n",
    "                np.random.choice([0, 1, 2], size=y_pred[:-1].shape, p=p),\n",
    "                X_ret[1:],\n",
    "                dtype=dtype\n",
    "            )[:,np.newaxis] for i in range(50)\n",
    "        ], axis=-1\n",
    "    )\n",
    "    plt.plot(eval(f\"index_{tmp_type}\")[(lag-1):-(h+1)][1:], x.mean(axis=-1), label='random', color='gray')\n",
    "    plt.plot(eval(f\"index_{tmp_type}\")[(lag-1):-(h+1)][1:], x.mean(axis=-1) + x.std(axis=-1), label='random_q75', color='gray', linestyle='--')\n",
    "    plt.plot(eval(f\"index_{tmp_type}\")[(lag-1):-(h+1)][1:], cumulated_return(y_pred[:-1] * 0 + 2, X_ret[1:], dtype=dtype), label='buy&HODL')\n",
    "    plt.plot(eval(f\"index_{tmp_type}\")[(lag-1):-(h+1)][1:], cumulated_return(y[:-1], X_ret[1:], dtype=dtype), label='oracle')\n",
    "    plt.plot(eval(f\"index_{tmp_type}\")[(lag-1):-(h+1)][1:], cumulated_return(y, X_ret, dtype=dtype)[:-1], label='oracle')\n",
    "    plt.legend()\n",
    "    plt.semilogy()\n",
    "    plt.show()\n",
    "    econ_metrics(y_pred, X_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output2('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output2('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output2('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-geology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fxpred3",
   "language": "python",
   "name": "fxpred3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
