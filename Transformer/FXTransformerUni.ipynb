{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i6zxz6xZyWb2",
    "outputId": "a40e8d2a-f684-4a98-fb9e-237db53569be"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/10min Dataset Spot.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-31a22a01d3a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_fx_and_metric_data_wo_weekend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Schreibtisch/phdneu/paper1_fxpred/Transformer/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0mget_fx_and_metric_data_wo_weekend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Schreibtisch/phdneu/paper1_fxpred/Transformer/utils.py\u001b[0m in \u001b[0;36mget_fx_and_metric_data_wo_weekend\u001b[0;34m(pct_change, dtype, directory, sep, enddate)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10min Dataset Spot.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dates'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%d.%m.%y %H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dates'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/10min Dataset Spot.csv'"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "if platform.node() in ['msbq', 'msdai']:\n",
    "    os.chdir('/home/ms/github/fxpred')\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'Transformer'))\n",
    "else:\n",
    "    os.chdir('/home/RDC/fisertiz/Schreibtisch/phd/paper1_fxpred')\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'Transformer'))\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import os, datetime\n",
    "from utils import get_fx_and_metric_data_wo_weekend\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import MSE, MAE, MAPE\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykFSM9LT54LR"
   },
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Acgb31zy5lcD"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 256\n",
    "d_k = 256\n",
    "d_v = 256\n",
    "n_heads = 12\n",
    "ff_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnPzNUPK50Ki"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_fx_and_metric_data\n",
    "df = get_fx_and_metric_data_wo_weekend(dtype=np.float32, directory='../data')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['EURUSD BGNE Curncy Bid Close', 'EURUSD BGNE Curncy Ask Close']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'EURUSD BGNE Curncy Bid Close'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training, validation and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "i7_Uju9B5lqY",
    "outputId": "d99dba2b-8ac4-467c-da85-758445af29b1"
   },
   "outputs": [],
   "source": [
    "times = sorted(df.index.values)\n",
    "last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n",
    "last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n",
    "\n",
    "df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
    "df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
    "df_test = df[(df.index >= last_10pct)]\n",
    "\n",
    "# Convert pandas columns into arrays\n",
    "train_data = df_train.values\n",
    "sc = StandardScaler().fit(train_data)\n",
    "sc_target = StandardScaler().fit(df_train.loc[:, target].values.reshape(-1, 1))\n",
    "train_data = sc.transform(train_data)\n",
    "train_data_dates = df_train.index.values  \n",
    "\n",
    "val_data = sc.transform(df_val.values)\n",
    "val_data_dates = df_val.index.values  \n",
    "\n",
    "test_data = sc.transform(df_test.values)\n",
    "test_data_dates = df_test.index.values \n",
    "\n",
    "print('Training data shape: {}'.format(train_data.shape))\n",
    "print('Validation data shape: {}'.format(val_data.shape))\n",
    "print('Test data shape: {}'.format(test_data.shape))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diGgUw_f6fAI"
   },
   "source": [
    "## Plot daily changes of close prices and volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "colab_type": "code",
    "id": "MJCti1mt5lnL",
    "outputId": "5cd6ad72-c397-4271-cec6-17d354ed9264"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,12))\n",
    "st = fig.suptitle(\"Data Separation\", fontsize=20)\n",
    "st.set_y(0.95)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(train_data_dates, df_train[target], label='Training data')\n",
    "ax1.plot(val_data_dates, df_val[target], label='Validation data')\n",
    "ax1.plot(test_data_dates, df_test[target], label='Test data')\n",
    "ax1.set_xlabel('Dates')\n",
    "ax1.set_ylabel('Normalized Closing Returns')\n",
    "ax1.set_title(\"Close Price\", fontsize=18)\n",
    "ax1.legend(loc=\"best\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-9JDha_6l3L"
   },
   "source": [
    "## Create chunks of training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Ib8wOc-_5llL",
    "outputId": "60209809-af34-43a2-f22c-c42c7c538769",
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_id = list(df_train.columns).index(target)\n",
    "# Training data\n",
    "X_train, y_train = [], []\n",
    "for i in range(seq_len, len(train_data)):\n",
    "    X_train.append(train_data[i-seq_len:i])\n",
    "    y_train.append(train_data[:, target_id][i])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Validation data\n",
    "X_val, y_val = [], []\n",
    "for i in range(seq_len, len(val_data)):\n",
    "    X_val.append(val_data[i-seq_len:i])\n",
    "    y_val.append(val_data[:, target_id][i])\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Test data\n",
    "X_test, y_test = [], []\n",
    "for i in range(seq_len, len(test_data)):\n",
    "    X_test.append(test_data[i-seq_len:i])\n",
    "    y_test.append(test_data[:, target_id][i])    \n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "print('Training set shape', X_train.shape, y_train.shape)\n",
    "print('Validation set shape', X_val.shape, y_val.shape)\n",
    "print('Testing set shape' ,X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WhTPe6I6sDu"
   },
   "source": [
    "## TimeVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUawOlTD5ljR"
   },
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''Calculate linear and periodic time features'''\n",
    "        x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
    "        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "   \n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJZOW8d56wyJ"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Veb1wu_5lhE"
   },
   "outputs": [],
   "source": [
    "class SingleAttention(Layer):\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.key = Dense(self.d_k, \n",
    "                         input_shape=input_shape, \n",
    "                         kernel_initializer='glorot_uniform', \n",
    "                         bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.value = Dense(self.d_v, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out    \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "        self.linear = Dense(input_shape[0][-1], \n",
    "                            input_shape=input_shape, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear   \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.2, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
    "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
    "  \n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer \n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtBanXmQB4jF"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "n1z9GgwV5lZe",
    "outputId": "3a97e918-00c4-4aaf-c0d6-1fd821ce868c"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    '''Initialize time and transformer layers'''\n",
    "    time_embedding = Time2Vector(seq_len)\n",
    "    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "    '''Construct model'''\n",
    "    in_seq = Input(shape=(seq_len, input_shape))\n",
    "    x = time_embedding(in_seq)\n",
    "    x = Concatenate(axis=-1)([in_seq, x])\n",
    "    #   x = Concatenate(axis=-1)([Dense(128)(in_seq), x])\n",
    "    x = attn_layer1((x, x, x))\n",
    "    x = attn_layer2((x, x, x))\n",
    "    x = attn_layer3((x, x, x))\n",
    "    # x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "    # x = Dense(256, activation='relu')(x)\n",
    "    x = Flatten()(x)    \n",
    "    x = Dropout(0.02)(x)\n",
    "    x = Dense(64, activation='linear')(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=in_seq, outputs=out)\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mae', 'mape'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(df.shape[1])\n",
    "model.summary()\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint('../content/FXTransformerUni{epoch:04}.hdf5', \n",
    "                                               monitor='val_loss', \n",
    "                                               save_best_only=True, verbose=1\n",
    "                                             )\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=8,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")\n",
    "\n",
    "\n",
    "if True: \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size, \n",
    "                        epochs=200,\n",
    "                        callbacks=[callback],\n",
    "                        validation_data=(X_val, y_val))  \n",
    "    with open('../content/historyUni.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_last = sorted([i for i in os.listdir('../content') if 'hdf5' in i and 'FXTransformerUni' in i])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "n1z9GgwV5lZe",
    "outputId": "3a97e918-00c4-4aaf-c0d6-1fd821ce868c"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    os.path.join('../content', model_last),\n",
    "    custom_objects={\n",
    "        'Time2Vector': Time2Vector,\n",
    "        'SingleAttention': SingleAttention,\n",
    "        'MultiAttention': MultiAttention,\n",
    "        'TransformerEncoder': TransformerEncoder\n",
    "    }\n",
    ")\n",
    "with open('../content/historyUni.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "n1z9GgwV5lZe",
    "outputId": "3a97e918-00c4-4aaf-c0d6-1fd821ce868c"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "'''Calculate predictions and metrics'''\n",
    "\n",
    "#Calculate predication for training, validation and test data\n",
    "train_pred = model.predict(X_train)\n",
    "val_pred = model.predict(X_val)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval = model.evaluate(X_train, y_train, verbose=0)\n",
    "val_eval = model.evaluate(X_val, y_val, verbose=0)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(' ')\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
    "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\n",
    "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mde(y_true, y_pred):\n",
    "    return 1 - np.equal(y_true[1:] - y_true[:-1] >= 0, y_pred[1:] - y_true[:-1] >= 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sc_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "y_pred = sc_target.inverse_transform(train_pred)\n",
    "print(f'mse: {MSE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mae: {MAE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mape: {MAPE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mde: {mde(y.flatten(), y_pred.flatten())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sc_target.inverse_transform(y_val.reshape(-1, 1))\n",
    "y_pred = sc_target.inverse_transform(val_pred)\n",
    "print(f'mse: {MSE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mae: {MAE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mape: {MAPE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mde: {mde(y.flatten(), y_pred.flatten())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sc_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred = sc_target.inverse_transform(test_pred)\n",
    "print(f'mse: {MSE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mae: {MAE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mape: {MAPE(y.flatten(), y_pred.flatten()).numpy()}')\n",
    "print(f'mde: {mde(y.flatten(), y_pred.flatten())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "n1z9GgwV5lZe",
    "outputId": "3a97e918-00c4-4aaf-c0d6-1fd821ce868c"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "'''Display results'''\n",
    "\n",
    "fig = plt.figure(figsize=(15,20))\n",
    "st = fig.suptitle(\"Transformer + TimeEmbedding Model\", fontsize=22)\n",
    "st.set_y(0.92)\n",
    "\n",
    "#Plot training data results\n",
    "ax11 = fig.add_subplot(311)\n",
    "ax11.plot(train_data_dates, train_data[:, 1], label='EURUSD Closing Returns')\n",
    "ax11.plot(train_data_dates[seq_len:], train_pred, linewidth=3, label='Predicted EURUSD Closing Returns')\n",
    "ax11.set_title(\"Training Data\", fontsize=18)\n",
    "ax11.set_xlabel('Dates')\n",
    "ax11.set_ylabel('EURUSD Closing Returns')\n",
    "ax11.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "#Plot validation data results\n",
    "ax21 = fig.add_subplot(312)\n",
    "ax21.plot(val_data_dates, val_data[:, 1], label='EURUSD Closing Returns')\n",
    "ax21.plot(val_data_dates[seq_len:], val_pred, linewidth=3, label='Predicted EURUSD Closing Returns')\n",
    "ax21.set_title(\"Validation Data\", fontsize=18)\n",
    "ax21.set_xlabel('Dates')\n",
    "ax21.set_ylabel('EURUSD Closing Returns')\n",
    "ax21.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "#Plot test data results\n",
    "ax31 = fig.add_subplot(313)\n",
    "ax31.plot(test_data_dates, test_data[:, 1], label='EURUSD Closing Returns')\n",
    "ax31.plot(test_data_dates[seq_len:], test_pred, linewidth=3, label='Predicted EURUSD Closing Returns')\n",
    "ax31.set_title(\"Test Data\", fontsize=18)\n",
    "ax31.set_xlabel('Dates')\n",
    "ax31.set_ylabel('EURUSD Closing Returns')\n",
    "ax31.legend(loc=\"best\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QiCB_z-iCMMr"
   },
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "maSSTkJPCOE8",
    "outputId": "5a802a5b-bfc7-47c0-987a-8fca1ab84226"
   },
   "outputs": [],
   "source": [
    "'''Display model metrics'''\n",
    "\n",
    "fig = plt.figure(figsize=(15,20))\n",
    "st = fig.suptitle(\"Transformer + TimeEmbedding Model Metrics\", fontsize=22)\n",
    "st.set_y(0.92)\n",
    "\n",
    "#Plot model loss\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.plot(history['loss'], label='Training loss (MSE)')\n",
    "ax1.plot(history['val_loss'], label='Test loss (MSE)')\n",
    "ax1.set_title(\"Model loss\", fontsize=18)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "#Plot MAE\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.plot(history['mae'], label='Training MAE')\n",
    "ax2.plot(history['val_mae'], label='Test MAE')\n",
    "ax2.set_title(\"Model metric - Mean average error (MAE)\", fontsize=18)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean average error (MAE)')\n",
    "ax2.legend(loc=\"best\", fontsize=12)\n",
    "\n",
    "#Plot MAPE\n",
    "ax3 = fig.add_subplot(313)\n",
    "ax3.plot(history['mape'], label='Training MAPE')\n",
    "ax3.plot(history['val_mape'], label='Test MAPE')\n",
    "ax3.set_title(\"Model metric - Mean average percentage error (MAPE)\", fontsize=18)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Mean average percentage error (MAPE)')\n",
    "ax3.legend(loc=\"best\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cvzV1ArCOwK"
   },
   "source": [
    "## Model architecture overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7b--iZiKCO5l",
    "outputId": "fa273886-666a-4ead-8d9e-e12083d4907b"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"FXTransformerUni.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    expand_nested=True,\n",
    "    dpi=96,)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IBM_Transformer+TimeEmbedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
